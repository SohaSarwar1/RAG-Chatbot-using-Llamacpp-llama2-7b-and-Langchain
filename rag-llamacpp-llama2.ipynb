{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9537435,"sourceType":"datasetVersion","datasetId":5809298},{"sourceId":53020,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":44512,"modelId":61438}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain\\\n    langchain-community\\\n    langchain-core\\\n    gpt4all\\\n    langgraph\\\n    chromadb\\\n    sentence-transformers\\\n    langchain-huggingface","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-03T08:06:53.402395Z","iopub.execute_input":"2024-10-03T08:06:53.403015Z","iopub.status.idle":"2024-10-03T08:07:45.827185Z","shell.execute_reply.started":"2024-10-03T08:06:53.402965Z","shell.execute_reply":"2024-10-03T08:07:45.826234Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.0 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pandas","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:07:49.715180Z","iopub.execute_input":"2024-10-03T08:07:49.715957Z","iopub.status.idle":"2024-10-03T08:08:01.647419Z","shell.execute_reply.started":"2024-10-03T08:07:49.715917Z","shell.execute_reply":"2024-10-03T08:08:01.646403Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install chromadb","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:19:43.228744Z","iopub.execute_input":"2024-10-03T08:19:43.229153Z","iopub.status.idle":"2024-10-03T08:19:56.513662Z","shell.execute_reply.started":"2024-10-03T08:19:43.229114Z","shell.execute_reply":"2024-10-03T08:19:56.512499Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: chromadb in /opt/conda/lib/python3.10/site-packages (0.5.11)\nRequirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.2.2)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.9.2)\nRequirement already satisfied: chroma-hnswlib==0.7.6 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.7.6)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.111.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.6.6)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.12.2)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.19.2)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.25.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.19.1)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.4.0)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.62.2)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.2.0)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.12.3)\nRequirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (31.0.0)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.3.0)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.2)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (5.0.1)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.4)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.0)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (13.7.1)\nRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.1)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\nRequirement already satisfied: python-multipart>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (2.1.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.30.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nRequirement already satisfied: durationpy>=0.7 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<=7.1,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-proto==1.25.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-instrumentation==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: opentelemetry-util-http==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.0.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\nRequirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.25.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:19:57.601571Z","iopub.execute_input":"2024-10-03T08:19:57.602009Z","iopub.status.idle":"2024-10-03T08:22:04.169409Z","shell.execute_reply.started":"2024-10-03T08:19:57.601966Z","shell.execute_reply":"2024-10-03T08:22:04.168364Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3511083 sha256=915a1e58e933f4c3f3d19d35728513901060a9ca95e8ce6d6ae2d6711239f111\n  Stored in directory: /root/.cache/pip/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom langchain.document_loaders.csv_loader import CSVLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.llms import LlamaCpp\nfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:22:28.580827Z","iopub.execute_input":"2024-10-03T08:22:28.581235Z","iopub.status.idle":"2024-10-03T08:22:30.580722Z","shell.execute_reply.started":"2024-10-03T08:22:28.581193Z","shell.execute_reply":"2024-10-03T08:22:30.579662Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:26:45.591446Z","iopub.execute_input":"2024-10-03T08:26:45.592384Z","iopub.status.idle":"2024-10-03T08:27:00.319242Z","shell.execute_reply.started":"2024-10-03T08:26:45.592343Z","shell.execute_reply":"2024-10-03T08:27:00.317885Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"csv_file_path = \"/kaggle/input/amazonreviews/Amazon_Reviews.csv\"\nqa_data = pd.read_csv(csv_file_path)\n\nloader = CSVLoader(file_path=csv_file_path)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\nchunks = text_splitter.split_documents(docs)\n\nembedding_function = HuggingFaceEmbeddings(show_progress=True, multi_process=False)\n\nvector_store = FAISS.from_documents(documents=chunks, embedding=embedding_function)\nretriever = vector_store.as_retriever()\n\ntemplate = \"\"\"You are an assistant for specific knowledge query tasks.\n   Use the following pieces of retrieved context to answer the question.\n   If you don't know the answer, just say that you don't know.\n   Question: {question}\n   Context: {context}\n   Answer:\n   \"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\n\nn_gpu_layers = 32  # Metal set to 1 is enough.\nn_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n\nllm = LlamaCpp(\n    model_path=\"/kaggle/input/llama2-7b-chat/gguf/llama2gguf/1/llama-2-7b-chat.Q4_K_M.gguf\",\n    #model_path=\"Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\",\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    # model_path=\"Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\",\n    n_ctx=1024,\n    f16_kv=True,\n    #temperature=0.75,\n    #max_tokens=500,\n    # top_p=1,\n    callback_manager=callback_manager,\n    verbose=True,\n)\n\nrag_chain = (\n        {\"context\": retriever, \"question\": RunnablePassthrough()} \n        | prompt\n        | llm\n        | StrOutputParser()\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:27:08.894951Z","iopub.execute_input":"2024-10-03T08:27:08.895723Z","iopub.status.idle":"2024-10-03T08:27:42.632514Z","shell.execute_reply.started":"2024-10-03T08:27:08.895680Z","shell.execute_reply":"2024-10-03T08:27:42.631532Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"131d3172a88e478b9e0394bd90cf8c49"}},"metadata":{}},{"name":"stderr","text":"llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /kaggle/input/llama2-7b-chat/gguf/llama2gguf/1/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 3\nllm_load_vocab: token to piece cache size = 0.1684 MB\nllm_load_print_meta: format           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 4096\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta: n_embd_k_gqa     = 4096\nllm_load_print_meta: n_embd_v_gqa     = 4096\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-06\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 4096\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 6.74 B\nllm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \nllm_load_print_meta: general.name     = LLaMA v2\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: ggml ctx size =    0.14 MiB\nllm_load_tensors:        CPU buffer size =  3891.24 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 1024\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\nllama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\nllama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\nllama_new_context_with_model:        CPU compute buffer size =    98.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"What are the specs of s22 ultra?\"\nprint(rag_chain.invoke(query))","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:35:33.670117Z","iopub.execute_input":"2024-10-03T08:35:33.670792Z","iopub.status.idle":"2024-10-03T08:38:21.924632Z","shell.execute_reply.started":"2024-10-03T08:35:33.670739Z","shell.execute_reply":"2024-10-03T08:38:21.923529Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c765779c73b9446ea800b134f45a8567"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: 52 prefix-match hit, remaining 681 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":" The Samsung Galaxy S22's specs are:\n* Processor: Snapdragon 8 Gen 1 or Exynos 2200 (depending on the region)\n* RAM: 8GB\n* Storage: 128GB internal storage\n* Camera: 50MP primary camera, 10MP front-facing camera\n* Battery: 3700mAh battery\n* Other features: Fingerprint sensor, IP68 rating, stereo speakers, and USB Type-C.\nI don't know the answer to your question regarding the S22 Ultra as there is no information provided in the context you provided.","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =  107859.87 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /   681 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =  168198.73 ms /   834 tokens\n","output_type":"stream"},{"name":"stdout","text":" The Samsung Galaxy S22's specs are:\n* Processor: Snapdragon 8 Gen 1 or Exynos 2200 (depending on the region)\n* RAM: 8GB\n* Storage: 128GB internal storage\n* Camera: 50MP primary camera, 10MP front-facing camera\n* Battery: 3700mAh battery\n* Other features: Fingerprint sensor, IP68 rating, stereo speakers, and USB Type-C.\nI don't know the answer to your question regarding the S22 Ultra as there is no information provided in the context you provided.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Can you tell me about LLMs?\"\nprint(rag_chain.invoke(query))","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:38:28.324649Z","iopub.execute_input":"2024-10-03T08:38:28.325027Z","iopub.status.idle":"2024-10-03T08:41:27.198710Z","shell.execute_reply.started":"2024-10-03T08:38:28.324994Z","shell.execute_reply":"2024-10-03T08:41:27.197801Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c01a8cb9dc1a4627aeeb2b7d7941501a"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: 51 prefix-match hit, remaining 568 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":" LLMS (Language Model Systems) are artificial intelligence systems trained on vast amounts of text data to perform various natural language processing tasks. They have become increasingly popular in recent years due to their ability to generate coherent and contextually relevant text.\nSome common applications of LLMs include:\n1. Language Translation: LLMs can be used to translate text from one language to another, allowing for more accurate and natural-sounding translations.\n2. Text Summarization: LLMs can summarize long pieces of text into shorter, more digestible versions while preserving the essential information.\n3. Sentiment Analysis: LLMs can analyze text to determine the sentiment or emotional tone behind it, such as detecting whether a piece of text is positive, negative, or neutral.\n4. Chatbots and Conversational AI: LLMs can be used to generate responses to user input in chatbots and other conversational interfaces, allowing for more natural-sounding and engaging interactions.\n5. Text Generation: LLMs can be used to generate coherent and contextually relevant text, such as answering questions or completing incomplete sentences. This has numerous applications, including content","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =  107859.87 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /   568 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =  178816.11 ms /   823 tokens\n","output_type":"stream"},{"name":"stdout","text":" LLMS (Language Model Systems) are artificial intelligence systems trained on vast amounts of text data to perform various natural language processing tasks. They have become increasingly popular in recent years due to their ability to generate coherent and contextually relevant text.\nSome common applications of LLMs include:\n1. Language Translation: LLMs can be used to translate text from one language to another, allowing for more accurate and natural-sounding translations.\n2. Text Summarization: LLMs can summarize long pieces of text into shorter, more digestible versions while preserving the essential information.\n3. Sentiment Analysis: LLMs can analyze text to determine the sentiment or emotional tone behind it, such as detecting whether a piece of text is positive, negative, or neutral.\n4. Chatbots and Conversational AI: LLMs can be used to generate responses to user input in chatbots and other conversational interfaces, allowing for more natural-sounding and engaging interactions.\n5. Text Generation: LLMs can be used to generate coherent and contextually relevant text, such as answering questions or completing incomplete sentences. This has numerous applications, including content\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"What fualts can be faced in s22?\"\nprint(rag_chain.invoke(query))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"What improvements can be made in s22 for its better selling?\"\nprint(rag_chain.invoke(query))","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:47:52.672515Z","iopub.execute_input":"2024-10-03T08:47:52.673011Z","iopub.status.idle":"2024-10-03T08:51:10.877862Z","shell.execute_reply.started":"2024-10-03T08:47:52.672968Z","shell.execute_reply":"2024-10-03T08:51:10.876922Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8527877780c4a42ab44ccc35d96633e"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: 51 prefix-match hit, remaining 639 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":" Based on the provided context, it appears that there are several improvements that could be made to improve the selling of the Samsung Galaxy S22:\n1. Better battery life: Many reviewers have noted that the battery life of the S22 can be a drawback, particularly if used heavily throughout the day. Improving the battery capacity or implementing more efficient power management features could help address this issue.\n2. Enhanced fingerprint sensor functionality: The fingerprint sensor on the S22 has been criticized for its poor accuracy and frequent lockouts. Investing in a better sensor technology or improving the current one's algorithms could help improve user experience.\n3. Expandable storage options: Some reviewers have noted that the limited internal storage capacity of the S22 can be a downside, especially for users who want to store more apps, photos, and videos. Providing expandable storage options like a microSD card slot or cloud storage integration could help address this issue.\n4. Improved water resistance: While the S22 has a water-resistant design, some reviewers have noted that it can still be susceptible to water damage if not properly cared for. Invest","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =  107859.87 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /   639 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =  198153.08 ms /   894 tokens\n","output_type":"stream"},{"name":"stdout","text":" Based on the provided context, it appears that there are several improvements that could be made to improve the selling of the Samsung Galaxy S22:\n1. Better battery life: Many reviewers have noted that the battery life of the S22 can be a drawback, particularly if used heavily throughout the day. Improving the battery capacity or implementing more efficient power management features could help address this issue.\n2. Enhanced fingerprint sensor functionality: The fingerprint sensor on the S22 has been criticized for its poor accuracy and frequent lockouts. Investing in a better sensor technology or improving the current one's algorithms could help improve user experience.\n3. Expandable storage options: Some reviewers have noted that the limited internal storage capacity of the S22 can be a downside, especially for users who want to store more apps, photos, and videos. Providing expandable storage options like a microSD card slot or cloud storage integration could help address this issue.\n4. Improved water resistance: While the S22 has a water-resistant design, some reviewers have noted that it can still be susceptible to water damage if not properly cared for. Invest\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"What is the most major fault in s22?\"\nprint(rag_chain.invoke(query))","metadata":{"execution":{"iopub.status.busy":"2024-10-03T08:52:13.518836Z","iopub.execute_input":"2024-10-03T08:52:13.519241Z","iopub.status.idle":"2024-10-03T08:54:10.240441Z","shell.execute_reply.started":"2024-10-03T08:52:13.519197Z","shell.execute_reply":"2024-10-03T08:54:10.239478Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"283730233bd14a328c015c21c1a759ed"}},"metadata":{}},{"name":"stderr","text":"Llama.generate: 52 prefix-match hit, remaining 601 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":" I don't know the answer to your question as none of the provided context contains information about the most major fault in s22.","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =  107859.87 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /   601 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =  116660.83 ms /   629 tokens\n","output_type":"stream"},{"name":"stdout","text":" I don't know the answer to your question as none of the provided context contains information about the most major fault in s22.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}